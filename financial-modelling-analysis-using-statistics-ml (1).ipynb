{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:22.038043Z",
     "iopub.status.busy": "2024-07-02T21:09:22.037682Z",
     "iopub.status.idle": "2024-07-02T21:09:22.419361Z",
     "shell.execute_reply": "2024-07-02T21:09:22.418538Z",
     "shell.execute_reply.started": "2024-07-02T21:09:22.038017Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:30:38.306434Z",
     "iopub.status.busy": "2024-07-02T22:30:38.306067Z",
     "iopub.status.idle": "2024-07-02T22:30:38.507958Z",
     "shell.execute_reply": "2024-07-02T22:30:38.50697Z",
     "shell.execute_reply.started": "2024-07-02T22:30:38.306408Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/company-bankruptcy-prediction/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:31:24.804977Z",
     "iopub.status.busy": "2024-07-02T22:31:24.80438Z",
     "iopub.status.idle": "2024-07-02T22:31:24.812991Z",
     "shell.execute_reply": "2024-07-02T22:31:24.81177Z",
     "shell.execute_reply.started": "2024-07-02T22:31:24.804947Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:22.929862Z",
     "iopub.status.busy": "2024-07-02T21:09:22.929527Z",
     "iopub.status.idle": "2024-07-02T21:09:22.943338Z",
     "shell.execute_reply": "2024-07-02T21:09:22.942463Z",
     "shell.execute_reply.started": "2024-07-02T21:09:22.92983Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Bankrupt?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:22.952117Z",
     "iopub.status.busy": "2024-07-02T21:09:22.95184Z",
     "iopub.status.idle": "2024-07-02T21:09:25.087493Z",
     "shell.execute_reply": "2024-07-02T21:09:25.086579Z",
     "shell.execute_reply.started": "2024-07-02T21:09:22.952075Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create a histogram\n",
    "fig = px.histogram(df, x='Bankrupt?', title='Distribution of bankrupt Column')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.089347Z",
     "iopub.status.busy": "2024-07-02T21:09:25.08904Z",
     "iopub.status.idle": "2024-07-02T21:09:25.27132Z",
     "shell.execute_reply": "2024-07-02T21:09:25.270528Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.089322Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/company-bankruptcy-prediction/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.272541Z",
     "iopub.status.busy": "2024-07-02T21:09:25.272287Z",
     "iopub.status.idle": "2024-07-02T21:09:25.281962Z",
     "shell.execute_reply": "2024-07-02T21:09:25.281086Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.272519Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and label\n",
    "X = df.drop('Bankrupt?', axis=1)\n",
    "y = df['Bankrupt?']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.284361Z",
     "iopub.status.busy": "2024-07-02T21:09:25.284073Z",
     "iopub.status.idle": "2024-07-02T21:09:25.304825Z",
     "shell.execute_reply": "2024-07-02T21:09:25.304123Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.284338Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets+\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.306104Z",
     "iopub.status.busy": "2024-07-02T21:09:25.30584Z",
     "iopub.status.idle": "2024-07-02T21:09:25.482681Z",
     "shell.execute_reply": "2024-07-02T21:09:25.481917Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.306069Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.clip(X, self.lower_bounds, self.upper_bounds)\n",
    "\n",
    "class LogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.log1p(X)\n",
    "\n",
    "# Initialize the VarianceThreshold object with a threshold of 0\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Define the preprocessing pipeline without the PCA step for feature name extraction\n",
    "preprocessing_pipeline_no_pca = Pipeline(steps=[\n",
    "    ('outlier_capper', OutlierCapper()),\n",
    "    ('constant_filter', constant_filter),\n",
    "    ('log_transform', LogTransformer()),\n",
    "    ('scaler', scaler)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessing_pipeline_no_pca.fit(X_train)\n",
    "\n",
    "# Get the feature names after the scaler step\n",
    "X_train_processed_no_pca = preprocessing_pipeline_no_pca.transform(X_train) #Rhis is the output without Applying PCA\n",
    "X_test_processed_no_pca = preprocessing_pipeline_no_pca.transform(X_test)\n",
    "\n",
    "# The feature names after the scaler step\n",
    "feature_names_after_scaler = X_train.columns[constant_filter.get_support()]\n",
    "\n",
    "# print(f'Feature names after scaler step: {feature_names_after_scaler}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:21:13.585538Z",
     "iopub.status.busy": "2024-07-02T22:21:13.585169Z",
     "iopub.status.idle": "2024-07-02T22:21:13.594003Z",
     "shell.execute_reply": "2024-07-02T22:21:13.592911Z",
     "shell.execute_reply.started": "2024-07-02T22:21:13.58551Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(feature_names_after_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PCA on Preprocessing for feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.484369Z",
     "iopub.status.busy": "2024-07-02T21:09:25.483931Z",
     "iopub.status.idle": "2024-07-02T21:09:25.796166Z",
     "shell.execute_reply": "2024-07-02T21:09:25.794849Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.484333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine steps into a full preprocessing pipeline including PCA\n",
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('outlier_capper', OutlierCapper()),\n",
    "    ('constant_filter', constant_filter),\n",
    "    ('log_transform', LogTransformer()),\n",
    "    ('scaler', scaler),\n",
    "    ('pca', pca)\n",
    "])\n",
    "\n",
    "# Fit the full preprocessing pipeline on the training data\n",
    "preprocessing_pipeline.fit(X_train)\n",
    "\n",
    "# Transform the training data\n",
    "X_train_processed = preprocessing_pipeline.transform(X_train)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
    "# Get the PCA components\n",
    "pca_components = pca.components_\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "pca_df = pd.DataFrame(pca_components, columns=feature_names_after_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:21:43.377116Z",
     "iopub.status.busy": "2024-07-02T22:21:43.376133Z",
     "iopub.status.idle": "2024-07-02T22:21:43.409328Z",
     "shell.execute_reply": "2024-07-02T22:21:43.408188Z",
     "shell.execute_reply.started": "2024-07-02T22:21:43.377065Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.798798Z",
     "iopub.status.busy": "2024-07-02T21:09:25.798038Z",
     "iopub.status.idle": "2024-07-02T21:09:25.804361Z",
     "shell.execute_reply": "2024-07-02T21:09:25.803257Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.79875Z"
    }
   },
   "outputs": [],
   "source": [
    "no_pca_df = pd.DataFrame(X_train_processed_no_pca, columns=feature_names_after_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:22:01.294646Z",
     "iopub.status.busy": "2024-07-02T22:22:01.293926Z",
     "iopub.status.idle": "2024-07-02T22:22:01.32176Z",
     "shell.execute_reply": "2024-07-02T22:22:01.320761Z",
     "shell.execute_reply.started": "2024-07-02T22:22:01.294614Z"
    }
   },
   "outputs": [],
   "source": [
    "no_pca_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:25.806723Z",
     "iopub.status.busy": "2024-07-02T21:09:25.806013Z",
     "iopub.status.idle": "2024-07-02T21:09:25.814574Z",
     "shell.execute_reply": "2024-07-02T21:09:25.813473Z",
     "shell.execute_reply.started": "2024-07-02T21:09:25.806678Z"
    }
   },
   "outputs": [],
   "source": [
    "no_pca_df_test=pd.DataFrame(X_test_processed_no_pca, columns=feature_names_after_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:22:12.002557Z",
     "iopub.status.busy": "2024-07-02T22:22:12.001944Z",
     "iopub.status.idle": "2024-07-02T22:22:12.012552Z",
     "shell.execute_reply": "2024-07-02T22:22:12.011545Z",
     "shell.execute_reply.started": "2024-07-02T22:22:12.002524Z"
    }
   },
   "outputs": [],
   "source": [
    "n_pcs = pca.components_.shape[0]\n",
    "\n",
    "# Get the index of the most important feature on EACH component\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "\n",
    "# Initial feature names after the scaler step\n",
    "initial_feature_names = feature_names_after_scaler.copy()\n",
    "\n",
    "# Get the names of the most important features\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a dictionary with PCA component, most important feature, and explained variance\n",
    "dic = {'PC{}'.format(i): {'Most Important Feature': most_important_names[i], 'Explained Variance': explained_variance[i]} for i in range(n_pcs)}\n",
    "\n",
    "# Build the dataframe\n",
    "df = pd.DataFrame(dic).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:23:13.649602Z",
     "iopub.status.busy": "2024-07-02T22:23:13.649238Z",
     "iopub.status.idle": "2024-07-02T22:23:13.659125Z",
     "shell.execute_reply": "2024-07-02T22:23:13.658232Z",
     "shell.execute_reply.started": "2024-07-02T22:23:13.649573Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10) #pca explaining variance of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:24:00.745553Z",
     "iopub.status.busy": "2024-07-02T22:24:00.745199Z",
     "iopub.status.idle": "2024-07-02T22:24:00.777012Z",
     "shell.execute_reply": "2024-07-02T22:24:00.776136Z",
     "shell.execute_reply.started": "2024-07-02T22:24:00.745523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the number of components\n",
    "n_components = pca.n_components_\n",
    "\n",
    "# Create names for the components\n",
    "component_names = [f'PC{i+1}' for i in range(n_components)]\n",
    "\n",
    "# Convert X_train_processed to a DataFrame\n",
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=component_names, index=X_train.index)\n",
    "\n",
    "# Create a DataFrame showing the contribution of each original feature to each principal component\n",
    "components_df = pd.DataFrame(pca.components_.T, columns=component_names, index=feature_names_after_scaler)\n",
    "\n",
    "print(\"Shape of X_train_processed_df:\", X_train_processed_df.shape)\n",
    "# print(X_train_processed_df.head())\n",
    "components_df.head(10)\n",
    "#Pca Components for each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Variance Explained by pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:27.055267Z",
     "iopub.status.busy": "2024-07-02T21:09:27.054727Z",
     "iopub.status.idle": "2024-07-02T21:09:27.633736Z",
     "shell.execute_reply": "2024-07-02T21:09:27.632822Z",
     "shell.execute_reply.started": "2024-07-02T21:09:27.055241Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = df['Explained Variance'].cumsum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_variance, marker='o', linestyle='-', color='b')\n",
    "plt.title('Cumulative Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.xticks(range(0, len(df)), df.index,rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we can use the approach of Oversampling+Undersampling given by the Research Papers to verify which Sampling technique is best for our dataset  [1](#ref1) [2](#ref2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used X_train and y train without preprocessing because the relative difference between all the algorithms wouldn't change much even if we used preprocessed inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:09:28.146564Z",
     "iopub.status.busy": "2024-07-02T21:09:28.146198Z",
     "iopub.status.idle": "2024-07-02T21:10:36.776131Z",
     "shell.execute_reply": "2024-07-02T21:10:36.775161Z",
     "shell.execute_reply.started": "2024-07-02T21:09:28.146528Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "\n",
    "\n",
    "# Define the techniques to evaluate\n",
    "oversampling_methods = [SMOTE(sampling_strategy=0.5), ADASYN(sampling_strategy=0.5)]\n",
    "undersampling_methods = [RandomUnderSampler(sampling_strategy=0.8), EditedNearestNeighbours()]\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the F2 scorer, F2 score is nothing but a variant of F1 score, Higher(2) F2 score weighs more on precision whereas lower(0 to 1) weighs more on recall. Here we are using beta as 2 so\n",
    "# the overall impact on F2 score will be relatively more determined by precision.\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Evaluate each combination\n",
    "results = []\n",
    "for over in oversampling_methods:\n",
    "    for under in undersampling_methods:\n",
    "        steps = [('over', over), ('under', under)]\n",
    "        pipeline = Pipeline(steps=steps)\n",
    "        \n",
    "        # Apply the pipeline to the training data\n",
    "        X_temp_resampled, y_temp_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model using cross-validation\n",
    "        scores = cross_val_score(model, X_temp_resampled, y_temp_resampled, scoring=f2_scorer, cv=5)\n",
    "        results.append({\n",
    "            'oversampler': over.__class__.__name__,\n",
    "            'undersampler': under.__class__.__name__,\n",
    "            'mean_f2_score': scores.mean()\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for easy comparison\n",
    "results_sample_df = pd.DataFrame(results)\n",
    "results_sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Technique would be SMOTE + RandomUnderSampling which we already applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:36.778457Z",
     "iopub.status.busy": "2024-07-02T21:10:36.778065Z",
     "iopub.status.idle": "2024-07-02T21:10:36.788585Z",
     "shell.execute_reply": "2024-07-02T21:10:36.787683Z",
     "shell.execute_reply.started": "2024-07-02T21:10:36.778424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get feature names after each step\n",
    "def get_feature_names_after_pipeline(pipeline, feature_names):\n",
    "    for name, step in pipeline.named_steps.items():\n",
    "        if hasattr(step, 'get_feature_names_out'):\n",
    "            feature_names = step.get_feature_names_out(feature_names)\n",
    "        elif hasattr(step, 'get_support'):\n",
    "            feature_names = np.array(feature_names)[step.get_support()]\n",
    "#         print(f'After {name}: {feature_names}')\n",
    "    return feature_names\n",
    "\n",
    "# Assuming original feature names are provided\n",
    "original_feature_names = X_train.columns  \n",
    "\n",
    "# Print feature names after each step in the preprocessing pipeline\n",
    "final_feature_names = get_feature_names_after_pipeline(preprocessing_pipeline, original_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:34:38.493201Z",
     "iopub.status.busy": "2024-07-02T21:34:38.492523Z",
     "iopub.status.idle": "2024-07-02T21:34:38.498237Z",
     "shell.execute_reply": "2024-07-02T21:34:38.497356Z",
     "shell.execute_reply.started": "2024-07-02T21:34:38.493171Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'Final feature names: {final_feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:36.789987Z",
     "iopub.status.busy": "2024-07-02T21:10:36.789703Z",
     "iopub.status.idle": "2024-07-02T21:10:36.804739Z",
     "shell.execute_reply": "2024-07-02T21:10:36.803913Z",
     "shell.execute_reply.started": "2024-07-02T21:10:36.789965Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the PCA components\n",
    "\n",
    "pca_components = pca.components_\n",
    "\n",
    "# # Create a DataFrame for better readability\n",
    "# pca_df = pd.DataFrame(pca_components, columns=X_train.columns)\n",
    "# print(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:36.806893Z",
     "iopub.status.busy": "2024-07-02T21:10:36.806629Z",
     "iopub.status.idle": "2024-07-02T21:10:36.818162Z",
     "shell.execute_reply": "2024-07-02T21:10:36.817315Z",
     "shell.execute_reply.started": "2024-07-02T21:10:36.80687Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_components.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for MultiColinearity, We can use either VIF,Correlation Matrix, Regularization\n",
    "### Here we will be using ridge regularization to select best features and also remove multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:36.846001Z",
     "iopub.status.busy": "2024-07-02T21:10:36.845722Z",
     "iopub.status.idle": "2024-07-02T21:10:37.154083Z",
     "shell.execute_reply": "2024-07-02T21:10:37.152737Z",
     "shell.execute_reply.started": "2024-07-02T21:10:36.845977Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "grid_search = GridSearchCV(estimator=Ridge(), param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_processed_no_pca, y_train)\n",
    "best_alpha = grid_search.best_params_['alpha']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.157436Z",
     "iopub.status.busy": "2024-07-02T21:10:37.15601Z",
     "iopub.status.idle": "2024-07-02T21:10:37.179452Z",
     "shell.execute_reply": "2024-07-02T21:10:37.178259Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.157389Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_ridge_regression(X, y, alpha=1.0):\n",
    "    ridge_reg = Ridge(alpha=alpha)\n",
    "    ridge_reg.fit(X, y)\n",
    "    return ridge_reg.coef_\n",
    "coefficients = apply_ridge_regression(X_train_processed_no_pca, y_train, alpha=best_alpha)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.182605Z",
     "iopub.status.busy": "2024-07-02T21:10:37.181177Z",
     "iopub.status.idle": "2024-07-02T21:10:37.191753Z",
     "shell.execute_reply": "2024-07-02T21:10:37.190543Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.182559Z"
    }
   },
   "outputs": [],
   "source": [
    "regularized_features=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We take the Middle Threshold here and select all the features lying inside this threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_preprocessed and coefficients are already defined\n",
    "feature_names = no_pca_df.columns\n",
    "# Create a DataFrame to store features and their coefficients\n",
    "regularized_features = pd.DataFrame({\n",
    "    'Features': feature_names,\n",
    "    'Coef': coefficients\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by coefficient values in descending order of absolute values\n",
    "# but keep the original sign\n",
    "regularized_features = regularized_features.reindex(\n",
    "    regularized_features['Coef'].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "# Function to select features based on a threshold, considering sign\n",
    "def select_features(df, threshold):\n",
    "    return df[(df['Coef'] >= threshold) | (df['Coef'] <= -threshold)]\n",
    "\n",
    "# Select features with different thresholds\n",
    "threshold_high = 0.02\n",
    "threshold_medium = 0.01\n",
    "threshold_low = 0.005\n",
    "\n",
    "\n",
    "\n",
    "selected_high = select_features(regularized_features, threshold_high)\n",
    "selected_medium = select_features(regularized_features, threshold_medium)\n",
    "selected_low = select_features(regularized_features, threshold_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:32:15.286733Z",
     "iopub.status.busy": "2024-07-02T21:32:15.286111Z",
     "iopub.status.idle": "2024-07-02T21:32:15.325951Z",
     "shell.execute_reply": "2024-07-02T21:32:15.32516Z",
     "shell.execute_reply.started": "2024-07-02T21:32:15.286704Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Print results\n",
    "display(Markdown('## Feature Coefficients Analysis'))\n",
    "display(Markdown('Without Caring about the Negative and Positive Values, we just checked the absolute values, i.e., the features which have the most impact on the target.'))\n",
    "\n",
    "display(Markdown('### Features with |coefficient| >= High Threshold'))\n",
    "display(selected_high[['Features', 'Coef']])\n",
    "\n",
    "display(Markdown('### Features with |coefficient| >= Medium Threshold'))\n",
    "display(selected_medium[['Features', 'Coef']])\n",
    "\n",
    "display(Markdown('### Features with |coefficient| >= Low Threshold'))\n",
    "display(selected_low[['Features', 'Coef']])\n",
    "\n",
    "# Get list of selected features (using medium threshold)\n",
    "selected_features = selected_medium['Features'].tolist()\n",
    "\n",
    "X_train_selected = no_pca_df[selected_features]\n",
    "X_test_selected = no_pca_df_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Pipeline classes for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.286849Z",
     "iopub.status.busy": "2024-07-02T21:10:37.285024Z",
     "iopub.status.idle": "2024-07-02T21:10:37.319298Z",
     "shell.execute_reply": "2024-07-02T21:10:37.318448Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.286803Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "class DataFrameVarianceThreshold(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=0):\n",
    "        self.threshold=threshold\n",
    "        self.variance_threshold = VarianceThreshold(threshold=self.threshold)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.variance_threshold.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_filtered = self.variance_threshold.transform(X)\n",
    "        return pd.DataFrame(X_filtered, columns=X.columns[self.variance_threshold.get_support()])\n",
    "    \n",
    "class DataFrameScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    \n",
    "class DF_log_transform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure output is DataFrame with original columns\n",
    "        return pd.DataFrame(np.log1p(X), columns=X.columns)\n",
    "\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, coefficients, thresholds):\n",
    "        self.coefficients = coefficients\n",
    "        self.thresholds = thresholds\n",
    "        self.df=df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        feature_names = X.columns\n",
    "        # Create a DataFrame to store features and their coefficients\n",
    "        self.regularized_features = pd.DataFrame({\n",
    "            'Features': feature_names,\n",
    "            'Coef': self.coefficients\n",
    "        })\n",
    "        # Sort the DataFrame by coefficient values in descending order of absolute values\n",
    "        self.regularized_features = self.regularized_features.reindex(\n",
    "            self.regularized_features['Coef'].abs().sort_values(ascending=False).index\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        selected_features = self.select_features(self.regularized_features, self.thresholds)\n",
    "        return X[selected_features['Features']]\n",
    "\n",
    "    def select_features(self, df, threshold):\n",
    "        return df[(df['Coef'] >= threshold) | (df['Coef'] <= -threshold)]\n",
    "\n",
    "\n",
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=1, upper_percentile=99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_capped = np.clip(X, self.lower_bounds, self.upper_bounds)\n",
    "        return pd.DataFrame(X_capped, columns=X.columns, index=X.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.321004Z",
     "iopub.status.busy": "2024-07-02T21:10:37.320432Z",
     "iopub.status.idle": "2024-07-02T21:10:37.332432Z",
     "shell.execute_reply": "2024-07-02T21:10:37.33167Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.32098Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the thresholds for regularization coeff\n",
    "thresholds = {\n",
    "    'high': 0.02,\n",
    "    'medium': 0.01,\n",
    "    'low': 0.005\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.33385Z",
     "iopub.status.busy": "2024-07-02T21:10:37.333604Z",
     "iopub.status.idle": "2024-07-02T21:10:37.466856Z",
     "shell.execute_reply": "2024-07-02T21:10:37.465883Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.333829Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the preprocessing pipeline without the PCA step for feature name extraction\n",
    "preprocessing_pipeline_final = Pipeline(steps=[\n",
    "    ('outlier_capper', OutlierCapper()),\n",
    "    ('constant_filter', DataFrameVarianceThreshold()),\n",
    "    ('log_transform', DF_log_transform()),\n",
    "    ('scaler', DataFrameScaler()),  # Use the custom DataFrameScaler\n",
    "    ('feature_selector', FeatureSelector(coefficients=coefficients, thresholds=thresholds['medium']))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessing_pipeline_final.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.468131Z",
     "iopub.status.busy": "2024-07-02T21:10:37.467839Z",
     "iopub.status.idle": "2024-07-02T21:10:37.56531Z",
     "shell.execute_reply": "2024-07-02T21:10:37.564245Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.468086Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Transform the training and test data using the updated pipeline\n",
    "X_train_Final = preprocessing_pipeline_final.transform(X_train)\n",
    "X_test_Final = preprocessing_pipeline_final.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.566787Z",
     "iopub.status.busy": "2024-07-02T21:10:37.566497Z",
     "iopub.status.idle": "2024-07-02T21:10:37.595925Z",
     "shell.execute_reply": "2024-07-02T21:10:37.595089Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.566765Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.597386Z",
     "iopub.status.busy": "2024-07-02T21:10:37.597112Z",
     "iopub.status.idle": "2024-07-02T21:10:37.87279Z",
     "shell.execute_reply": "2024-07-02T21:10:37.871866Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.597364Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Initialize SMOTEENN\n",
    "smote_enn = SMOTEENN()\n",
    "\n",
    "# Apply SMOTEENN to the processed training data\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train_Final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Distribution of Data in each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:37.874629Z",
     "iopub.status.busy": "2024-07-02T21:10:37.874071Z",
     "iopub.status.idle": "2024-07-02T21:10:46.975148Z",
     "shell.execute_reply": "2024-07-02T21:10:46.974135Z",
     "shell.execute_reply.started": "2024-07-02T21:10:37.874603Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "num_features = X_train_Final.shape[1]\n",
    "num_rows = (num_features - 1) // 6 + 1 \n",
    "plt.figure(figsize=(20, 5 * num_rows))\n",
    "\n",
    "for i, feature in enumerate(X_train_Final.columns):\n",
    "    plt.subplot(num_rows, 6, i + 1)\n",
    "    sns.histplot(X_train_Final[feature], kde=True, bins=30)\n",
    "    plt.title(f'{feature}')\n",
    "    plt.xlabel('')  \n",
    "    plt.yticks([])  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:10:46.976887Z",
     "iopub.status.busy": "2024-07-02T21:10:46.976471Z",
     "iopub.status.idle": "2024-07-02T21:10:47.145295Z",
     "shell.execute_reply": "2024-07-02T21:10:47.14453Z",
     "shell.execute_reply.started": "2024-07-02T21:10:46.976855Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, make_scorer, fbeta_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:37:27.001109Z",
     "iopub.status.busy": "2024-07-02T21:37:27.0004Z",
     "iopub.status.idle": "2024-07-02T21:37:27.011472Z",
     "shell.execute_reply": "2024-07-02T21:37:27.01034Z",
     "shell.execute_reply.started": "2024-07-02T21:37:27.001065Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42), {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__class_weight': ['balanced']\n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42), {\n",
    "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'model__solver': ['lbfgs', 'saga', 'newton-cg', 'sag'],\n",
    "        'model__class_weight': ['balanced']\n",
    "    }),\n",
    "    'SVM': (SVC(probability=True, random_state=42), {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__kernel': ['linear', 'rbf'],\n",
    "        'model__class_weight': ['balanced']\n",
    "    }),\n",
    "    'AdaBoost': (AdaBoostClassifier(random_state=42), {\n",
    "        'model__n_estimators': [50, 100],\n",
    "        'model__learning_rate': [0.01, 0.1, 1]\n",
    "    }),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(random_state=42), {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 1]\n",
    "    }),\n",
    "    'XGBoost': (XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 1]\n",
    "    }),\n",
    "    'ANN': (MLPClassifier(random_state=42, max_iter=500, early_stopping=True), {\n",
    "        'model__hidden_layer_sizes': [(100,), (50, 50)],\n",
    "        'model__activation': ['relu', 'tanh'],\n",
    "        'model__solver': ['adam', 'sgd']\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:37:35.38549Z",
     "iopub.status.busy": "2024-07-02T21:37:35.384737Z",
     "iopub.status.idle": "2024-07-02T21:44:51.572954Z",
     "shell.execute_reply": "2024-07-02T21:44:51.571937Z",
     "shell.execute_reply.started": "2024-07-02T21:37:35.385454Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Define the F2 scorer\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# # Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize results dataframe\n",
    "results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'])\n",
    "\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, scoring=f2_scorer, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = grid_search.predict(X_test_selected)\n",
    "    scores = grid_search.predict_proba(X_test_selected)[:, 1]\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, scores)\n",
    "    \n",
    "    # Append results\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC AUC': roc_auc\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    print(f\"{model_name} - Done\")\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n",
    "\n",
    "# Visualize results\n",
    "results_df.set_index('Model', inplace=True)\n",
    "results_df.plot(kind='bar', figsize=(14, 8))\n",
    "plt.title('Model Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:46:35.922067Z",
     "iopub.status.busy": "2024-07-02T21:46:35.921677Z",
     "iopub.status.idle": "2024-07-02T21:46:35.934774Z",
     "shell.execute_reply": "2024-07-02T21:46:35.933942Z",
     "shell.execute_reply.started": "2024-07-02T21:46:35.922037Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:47:00.63795Z",
     "iopub.status.busy": "2024-07-02T21:47:00.637087Z",
     "iopub.status.idle": "2024-07-02T21:47:00.672136Z",
     "shell.execute_reply": "2024-07-02T21:47:00.671248Z",
     "shell.execute_reply.started": "2024-07-02T21:47:00.637921Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.value_counts()\n",
    "baseline_predict = [0] * len(y_test)\n",
    "print(\"\\nBaseline Model:\")\n",
    "print(classification_report(y_test, baseline_predict,zero_division = 0))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, baseline_predict)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, baseline_predict)}\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, baseline_predict)\n",
    "precision = precision_score(y_test, baseline_predict)\n",
    "recall = recall_score(y_test, baseline_predict)\n",
    "f1 = f1_score(y_test, baseline_predict)\n",
    "roc_auc = roc_auc_score(y_test, baseline_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:52:35.298264Z",
     "iopub.status.busy": "2024-07-02T21:52:35.297439Z",
     "iopub.status.idle": "2024-07-02T21:52:35.305397Z",
     "shell.execute_reply": "2024-07-02T21:52:35.304389Z",
     "shell.execute_reply.started": "2024-07-02T21:52:35.298232Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "        'Model': 'Baseline Model',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC AUC': roc_auc\n",
    "    }]).set_index('Model')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:58:03.175271Z",
     "iopub.status.busy": "2024-07-02T21:58:03.174605Z",
     "iopub.status.idle": "2024-07-02T21:58:03.188175Z",
     "shell.execute_reply": "2024-07-02T21:58:03.187162Z",
     "shell.execute_reply.started": "2024-07-02T21:58:03.175241Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting every metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:02:26.349203Z",
     "iopub.status.busy": "2024-07-02T22:02:26.3484Z",
     "iopub.status.idle": "2024-07-02T22:02:26.461544Z",
     "shell.execute_reply": "2024-07-02T22:02:26.460615Z",
     "shell.execute_reply.started": "2024-07-02T22:02:26.349169Z"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import plotly.offline as pyo\n",
    "import plotly.express as px\n",
    "\n",
    "# Activate the offline mode for Plotly\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Reset the index to turn the model names into a column\n",
    "results_df.reset_index(inplace=True)\n",
    "results_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "\n",
    "# Melt the DataFrame to long format for Plotly\n",
    "results_melted = results_df.melt(id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "# Create the plot\n",
    "fig = px.bar(results_melted, x='Model', y='Value', color='Metric', barmode='group',\n",
    "             title='Model Performance Metrics',\n",
    "             labels={'Value': 'Score', 'Model': 'Model'})\n",
    "\n",
    "# Show the plot in Jupyter notebook\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:04:20.37301Z",
     "iopub.status.busy": "2024-07-02T22:04:20.372038Z",
     "iopub.status.idle": "2024-07-02T22:04:20.518993Z",
     "shell.execute_reply": "2024-07-02T22:04:20.518076Z",
     "shell.execute_reply.started": "2024-07-02T22:04:20.372976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Melt the DataFrame to long format for Plotly\n",
    "results_melted = results_df.melt(id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "# Create the line plot\n",
    "fig = px.line(results_melted, x='Metric', y='Value', color='Model', markers=True,\n",
    "              title='Model Performance Metrics',\n",
    "              labels={'Value': 'Score', 'Metric': 'Metric'})\n",
    "\n",
    "# Show the plot in Jupyter notebook\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:00:45.365481Z",
     "iopub.status.busy": "2024-07-02T22:00:45.365085Z",
     "iopub.status.idle": "2024-07-02T22:00:45.379612Z",
     "shell.execute_reply": "2024-07-02T22:00:45.378571Z",
     "shell.execute_reply.started": "2024-07-02T22:00:45.365454Z"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now lets focus on XGB and Random Forest we will be stacking them together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Tuning Using Optuna and then Stacking the models using LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:19:02.703429Z",
     "iopub.status.busy": "2024-07-02T21:19:02.703065Z",
     "iopub.status.idle": "2024-07-02T21:19:02.843226Z",
     "shell.execute_reply": "2024-07-02T21:19:02.842336Z",
     "shell.execute_reply.started": "2024-07-02T21:19:02.703397Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, make_scorer, fbeta_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:17:36.025987Z",
     "iopub.status.busy": "2024-07-02T22:17:36.025595Z",
     "iopub.status.idle": "2024-07-02T22:17:56.219838Z",
     "shell.execute_reply": "2024-07-02T22:17:56.218286Z",
     "shell.execute_reply.started": "2024-07-02T22:17:36.025958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress Optuna INFO messages\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def rf_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 10, 20)\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred_prob = model.predict_proba(X_test_selected)[:, 1]\n",
    "    \n",
    "    threshold = trial.suggest_float('threshold', 0.1, 0.9)\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return f1\n",
    "\n",
    "rf_study = optuna.create_study(direction='maximize')\n",
    "rf_study.optimize(rf_objective, n_trials=50)\n",
    "best_rf_params = rf_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:23:44.669987Z",
     "iopub.status.busy": "2024-07-02T21:23:44.669645Z",
     "iopub.status.idle": "2024-07-02T21:24:25.188694Z",
     "shell.execute_reply": "2024-07-02T21:24:25.187998Z",
     "shell.execute_reply.started": "2024-07-02T21:23:44.669953Z"
    }
   },
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 200)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "    model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
    "    \n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred_prob = model.predict_proba(X_test_selected)[:, 1]\n",
    "    \n",
    "    threshold = trial.suggest_float('threshold', 0.1, 0.9)\n",
    "    y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return f1\n",
    "\n",
    "xgb_study = optuna.create_study(direction='maximize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=50)\n",
    "best_xgb_params = xgb_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:24:25.190237Z",
     "iopub.status.busy": "2024-07-02T21:24:25.189732Z",
     "iopub.status.idle": "2024-07-02T21:24:58.53934Z",
     "shell.execute_reply": "2024-07-02T21:24:58.538525Z",
     "shell.execute_reply.started": "2024-07-02T21:24:25.190208Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=best_rf_params['n_estimators'],\n",
    "    max_depth=best_rf_params['max_depth'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=best_xgb_params['n_estimators'],\n",
    "    learning_rate=best_xgb_params['learning_rate'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Train the stacked model\n",
    "stacked_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict with optimized thresholds (use a common threshold for simplicity)\n",
    "threshold = max(best_rf_params['threshold'], best_xgb_params['threshold'])\n",
    "y_pred_prob = stacked_model.predict_proba(X_test_selected)[:, 1]\n",
    "y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Evaluate the stacked model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets study the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:29:29.209156Z",
     "iopub.status.busy": "2024-07-02T22:29:29.208777Z",
     "iopub.status.idle": "2024-07-02T22:29:29.271701Z",
     "shell.execute_reply": "2024-07-02T22:29:29.270855Z",
     "shell.execute_reply.started": "2024-07-02T22:29:29.20912Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Predict with optimized threshold\n",
    "y_pred_prob = stacked_model.predict_proba(X_test_Final)[:, 1]\n",
    "y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Print the confusion matrix and metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP - Type I Error): {fp}\")\n",
    "print(f\"False Negatives (FN - Type II Error): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T22:29:39.50682Z",
     "iopub.status.busy": "2024-07-02T22:29:39.506434Z",
     "iopub.status.idle": "2024-07-02T22:29:39.517931Z",
     "shell.execute_reply": "2024-07-02T22:29:39.516938Z",
     "shell.execute_reply.started": "2024-07-02T22:29:39.506792Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create the markdown string with variables\n",
    "markdown_content = f\"\"\"\n",
    "### Confusion Matrix\n",
    "- **True Negatives (TN):** {tn}\n",
    "  - The model correctly identified {tn} instances as non-bankrupt (0).\n",
    "\n",
    "- **False Positives (FP - Type I Error):** {fp}\n",
    "  - The model incorrectly classified {fp} non-bankrupt instances as bankrupt (1).\n",
    "\n",
    "- **False Negatives (FN - Type II Error):** {fn}\n",
    "  - The model incorrectly classified {fn} bankrupt instances as non-bankrupt (0).\n",
    "\n",
    "- **True Positives (TP):** {tp}\n",
    "  - The model correctly identified {tp} instances as bankrupt (1).\n",
    "\n",
    "### Performance Metrics\n",
    "- **Accuracy:** {accuracy:.4f}\n",
    "  - Accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined. It indicates that the model correctly classified {accuracy:.4f}% of the instances.\n",
    "\n",
    "- **Precision:** {precision:.4f}\n",
    "  - Precision is the proportion of true positive results in the predicted positive results. Here, {precision:.4f}% of the instances that the model classified as bankrupt were actually bankrupt. This is relatively low, indicating that there are many false positives.\n",
    "\n",
    "- **Recall (Sensitivity):** {recall:.4f}\n",
    "  - Recall is the proportion of true positive results in the actual positive results. The model correctly identified {recall:.4f}% of the bankrupt instances. This is quite good, indicating that the model is effective at catching most of the bankrupt instances.\n",
    "\n",
    "- **F1 Score:** {f1:.4f}\n",
    "  - The F1 score is the harmonic mean of precision and recall. It balances precision and recall, and here it is {f1:.4f}, indicating moderate performance in both aspects.\n",
    "\n",
    "- **ROC AUC:** {roc_auc:.4f}\n",
    "  - The ROC AUC score is a measure of how well the model distinguishes between the classes. A score of {roc_auc:.4f} indicates that the model has a high ability to distinguish between bankrupt and non-bankrupt instances.\n",
    "\"\"\"\n",
    "\n",
    "# Display the markdown content\n",
    "display(Markdown(markdown_content))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "### Interpretation\n",
    "- **High Accuracy and ROC AUC:**\n",
    "  - The model performs well overall, correctly classifying most instances and effectively distinguishing between bankrupt and non-bankrupt cases.\n",
    "\n",
    "- **High Recall, Low Precision:**\n",
    "  - The model has a high recall, meaning it successfully identifies most of the bankrupt cases. However, the precision is low, indicating that many of the instances it identifies as bankrupt are actually non-bankrupt. This results in a higher number of false positives .\n",
    "\n",
    "- **False Positives vs. False Negatives:**\n",
    "  - There are more false positives than false negatives . This means that the model is more likely to incorrectly classify a non-bankrupt instance as bankrupt than the other way around. In the context of bankruptcy prediction, this may be a cautious approach, ensuring that potential bankruptcies are flagged, even at the cost of some false alarms.\n",
    "\n",
    "- **Balancing Precision and Recall:**\n",
    "  - Given the importance of detecting bankruptcies (recall), the model's current threshold is achieving a good balance. However, depending on your application's tolerance for false positives, you might want to further adjust the threshold or employ additional strategies to improve precision without significantly impacting recall.\n",
    "\n",
    "# Conclusion\n",
    "- The model is good at identifying bankrupt instances (high recall), but there is a trade-off with precision, resulting in a notable number of false positives. The high ROC AUC indicates strong overall performance, suggesting that the model is well-calibrated for distinguishing between the classes. Further fine-tuning of the threshold or additional techniques might be employed to improve precision if false positives are a concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:24:58.618366Z",
     "iopub.status.busy": "2024-07-02T21:24:58.618117Z",
     "iopub.status.idle": "2024-07-02T21:24:58.628725Z",
     "shell.execute_reply": "2024-07-02T21:24:58.627883Z",
     "shell.execute_reply.started": "2024-07-02T21:24:58.618344Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class CombinedModel:\n",
    "    def __init__(self, preprocessing_pipeline, model, threshold=threshold):\n",
    "        self.preprocessing_pipeline = preprocessing_pipeline\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.data_storage = []  # To store incoming data for retraining\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_processed = self.preprocessing_pipeline.transform(X)\n",
    "        y_pred_prob = self.model.predict_proba(X_processed)[:, 1]\n",
    "        return (y_pred_prob >= self.threshold).astype(int)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_processed = self.preprocessing_pipeline.transform(X)\n",
    "        return self.model.predict_proba(X_processed)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            joblib.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            return joblib.load(f)\n",
    "\n",
    "    def store_data(self, X, y):\n",
    "        self.data_storage.append((X, y))\n",
    "\n",
    "    def get_stored_data(self):\n",
    "        X_data = np.concatenate([data[0] for data in self.data_storage], axis=0)\n",
    "        y_data = np.concatenate([data[1] for data in self.data_storage], axis=0)\n",
    "        return X_data, y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Pipeline as we created before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:24:58.630362Z",
     "iopub.status.busy": "2024-07-02T21:24:58.630033Z",
     "iopub.status.idle": "2024-07-02T21:24:58.759189Z",
     "shell.execute_reply": "2024-07-02T21:24:58.758309Z",
     "shell.execute_reply.started": "2024-07-02T21:24:58.630331Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessing_pipeline_final = Pipeline(steps=[\n",
    "    ('outlier_capper', OutlierCapper()),\n",
    "    ('constant_filter', DataFrameVarianceThreshold(threshold=0)),\n",
    "    ('log_transform', DF_log_transform()),\n",
    "    ('scaler', DataFrameScaler()),\n",
    "    ('feature_selector', FeatureSelector(coefficients=coefficients, thresholds=thresholds['medium']))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "preprocessing_pipeline_final.fit(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:25:50.048051Z",
     "iopub.status.busy": "2024-07-02T21:25:50.046723Z",
     "iopub.status.idle": "2024-07-02T21:25:50.176977Z",
     "shell.execute_reply": "2024-07-02T21:25:50.176221Z",
     "shell.execute_reply.started": "2024-07-02T21:25:50.048015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the combined model with the fitted model and preprocessing pipeline\n",
    "combined_model = CombinedModel(preprocessing_pipeline=preprocessing_pipeline_final, model=stacked_model, threshold=threshold)\n",
    "\n",
    "# Save the combined model\n",
    "combined_model.save('combined_model_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:25:50.205929Z",
     "iopub.status.busy": "2024-07-02T21:25:50.205262Z",
     "iopub.status.idle": "2024-07-02T21:25:50.435837Z",
     "shell.execute_reply": "2024-07-02T21:25:50.435124Z",
     "shell.execute_reply.started": "2024-07-02T21:25:50.205895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the combined model\n",
    "loaded_model = CombinedModel.load('combined_model_test.pkl')\n",
    "new_data=X_test.copy()\n",
    "\n",
    "predictions = loaded_model.predict(new_data)\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "# Interpreting Original Data with Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:25:54.507448Z",
     "iopub.status.busy": "2024-07-02T21:25:54.506533Z",
     "iopub.status.idle": "2024-07-02T21:25:55.83534Z",
     "shell.execute_reply": "2024-07-02T21:25:55.834357Z",
     "shell.execute_reply.started": "2024-07-02T21:25:54.507414Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X, y = X.align(y, join='inner', axis=0)\n",
    "\n",
    "X = sm.add_constant(X)  # Add a constant term to the model for mathematical convenience\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Create a summary dataframe\n",
    "summary_df = pd.DataFrame({\n",
    "    'Variable': X.columns,\n",
    "    'Coefficient': model.params,\n",
    "    'Std. Err.': model.bse,\n",
    "    't_statistic': model.tvalues,\n",
    "    'P>|z|': model.pvalues,\n",
    "    '[0.025': model.conf_int()[0],\n",
    "    '0.975]': model.conf_int()[1]\n",
    "})\n",
    "\n",
    "# Sort by p-value\n",
    "summary_df = summary_df.sort_values('P>|z|')\n",
    "\n",
    "# Print the summary\n",
    "summary_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The p value here if less than significance level is basically confirming that the variable/feature has large impact on the target but on the other hand it doesn't mean that other feature/variables have no impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:13.563564Z",
     "iopub.status.busy": "2024-07-02T21:26:13.562438Z",
     "iopub.status.idle": "2024-07-02T21:26:13.568776Z",
     "shell.execute_reply": "2024-07-02T21:26:13.56787Z",
     "shell.execute_reply.started": "2024-07-02T21:26:13.563524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set significance level\n",
    "alpha = 0.05\n",
    "summary_df['Significance'] = summary_df['P>|z|'].apply(lambda p: 'Reject H0' if p < alpha else '    Failed to reject H0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:14.328013Z",
     "iopub.status.busy": "2024-07-02T21:26:14.327401Z",
     "iopub.status.idle": "2024-07-02T21:26:14.33704Z",
     "shell.execute_reply": "2024-07-02T21:26:14.336132Z",
     "shell.execute_reply.started": "2024-07-02T21:26:14.327982Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:31.331385Z",
     "iopub.status.busy": "2024-07-02T21:26:31.330997Z",
     "iopub.status.idle": "2024-07-02T21:26:31.34085Z",
     "shell.execute_reply": "2024-07-02T21:26:31.339953Z",
     "shell.execute_reply.started": "2024-07-02T21:26:31.331355Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df=summary_df.reset_index().drop('index',axis=1)\n",
    "# Optionally, save to CSV\n",
    "summary_df.to_csv('financial_ratios_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:35.410006Z",
     "iopub.status.busy": "2024-07-02T21:26:35.409219Z",
     "iopub.status.idle": "2024-07-02T21:26:35.42688Z",
     "shell.execute_reply": "2024-07-02T21:26:35.425976Z",
     "shell.execute_reply.started": "2024-07-02T21:26:35.409972Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:40.645052Z",
     "iopub.status.busy": "2024-07-02T21:26:40.644716Z",
     "iopub.status.idle": "2024-07-02T21:26:40.676853Z",
     "shell.execute_reply": "2024-07-02T21:26:40.676142Z",
     "shell.execute_reply.started": "2024-07-02T21:26:40.645026Z"
    }
   },
   "outputs": [],
   "source": [
    "significant_summary_df=summary_df.loc[1:10]\n",
    "Descriptive_Stats=X[significant_summary_df.Variable].describe().T\n",
    "Descriptive_Stats['Skew']=X[significant_summary_df.Variable].skew()\n",
    "Descriptive_Stats['Kurtosis']=X[significant_summary_df.Variable].kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:40.744606Z",
     "iopub.status.busy": "2024-07-02T21:26:40.744329Z",
     "iopub.status.idle": "2024-07-02T21:26:40.750036Z",
     "shell.execute_reply": "2024-07-02T21:26:40.749144Z",
     "shell.execute_reply.started": "2024-07-02T21:26:40.744583Z"
    }
   },
   "outputs": [],
   "source": [
    "Descriptive_Stats.to_csv('Descriptive_Stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:49.865173Z",
     "iopub.status.busy": "2024-07-02T21:26:49.864299Z",
     "iopub.status.idle": "2024-07-02T21:26:49.885467Z",
     "shell.execute_reply": "2024-07-02T21:26:49.884575Z",
     "shell.execute_reply.started": "2024-07-02T21:26:49.865127Z"
    }
   },
   "outputs": [],
   "source": [
    "Descriptive_Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T00:21:43.946033Z",
     "iopub.status.busy": "2024-07-01T00:21:43.944645Z",
     "iopub.status.idle": "2024-07-01T00:21:43.992652Z",
     "shell.execute_reply": "2024-07-01T00:21:43.991194Z",
     "shell.execute_reply.started": "2024-07-01T00:21:43.945994Z"
    }
   },
   "source": [
    "### High kurtosis means basically thick tail and high peak, this can be indication of large majority of companies inside the common range and some of the countries at pretty high or low, In this data if we look at After-tax net Interest Rate and Pre-tax net Interest Rate, The Net Interest Rate is generally interest gained - interest paid after doing Revenue - (COGS+ Other Expenses), The quantity means that most of the companies had the similar range of net Interest rate but there were big amount of outliers which had very low or very high of this interest rate\n",
    "\n",
    "### Less than 3 Kurtosis means the distribution is moving towards Uniform distribution, Cash/Total Assets,Asset TurnOver and Current Assets/Total assets are almost uniform in most companies. Current Assets are Assets valuated or kept for low time period, usually less than 1 year like liquidity,Credit,deposits,Bonds and other financial instruments. Most of these companies in out data have equivalent amount of Current Assets/Total Assets which should make sense because most of the companies like to keep a specific ratio of fixed and current assets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:58.295389Z",
     "iopub.status.busy": "2024-07-02T21:26:58.29502Z",
     "iopub.status.idle": "2024-07-02T21:26:58.30347Z",
     "shell.execute_reply": "2024-07-02T21:26:58.302424Z",
     "shell.execute_reply.started": "2024-07-02T21:26:58.295362Z"
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix=X[significant_summary_df.Variable].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:26:58.66755Z",
     "iopub.status.busy": "2024-07-02T21:26:58.667239Z",
     "iopub.status.idle": "2024-07-02T21:26:58.685882Z",
     "shell.execute_reply": "2024-07-02T21:26:58.684796Z",
     "shell.execute_reply.started": "2024-07-02T21:26:58.667524Z"
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:27:11.667714Z",
     "iopub.status.busy": "2024-07-02T21:27:11.666891Z",
     "iopub.status.idle": "2024-07-02T21:27:11.671417Z",
     "shell.execute_reply": "2024-07-02T21:27:11.670436Z",
     "shell.execute_reply.started": "2024-07-02T21:27:11.667684Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.triu(correlation_matrix, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T21:27:13.82186Z",
     "iopub.status.busy": "2024-07-02T21:27:13.820817Z",
     "iopub.status.idle": "2024-07-02T21:27:14.50583Z",
     "shell.execute_reply": "2024-07-02T21:27:14.504787Z",
     "shell.execute_reply.started": "2024-07-02T21:27:13.821812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', vmax=1, vmin=-1, center=0, square=True, linewidths=.5)\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High correlation between After-Tax and Before-Tax interest rate is expected because \n",
    "The after-tax interest rate is calculated as: $After\\_Tax\\_Interest\\_Rate = Before\\_Tax\\_Interest \\cdot (1 - Tax\\_Rate)$\n",
    "\n",
    "## High correlation between Borrowing Dependency and Current Liability to Equity\n",
    "This also makes sense because the debt is considered as Liability and if u have more debt you have to pay back the debt within a year i.e increase of current Liability.\n",
    "\n",
    "## Net Income to StockHolder's Equity is inversely proportional to Borrowing Dependency and CL/Equity\n",
    "This relationship means the Net Income which is nothing but Assets - Liabilities in balance sheet or Revenue-COGS-Operating Costs-Interest-Taxes(Basically all the obligations of a company)/Shareholder's Equity(This is nothing but Equity which the Company carried with it from initial to current stage) will increase when The residual profit will increase(increase of profit margin) which will also make the Company reinvest or pay dividends to its shareholders or buy back stocks, This in short means a good healthy company.\n",
    "if that is the case then the company is not in debt and even if it is, it has the capability to pay back the debt whereas if this quantity is very low compared to Equity which the company holds as its core, then there are high chances of it borrowing Debt more often.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1111894,
     "sourceId": 1938459,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
